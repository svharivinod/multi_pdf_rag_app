# ğŸ“„ CiteRAG â€” Document Q&A with Citations

CiteRAG is a **Multi-PDF Retrieval-Augmented Generation (RAG) application** that allows users to upload multiple PDF documents, ask natural-language questions, and receive **grounded answers with clear page-level citations**.

The system ensures that answers are generated **only from the uploaded documents**, making it suitable for use cases that require **accuracy, traceability, and trust**.

<img width="1860" height="825" alt="CiteRAG" src="https://github.com/user-attachments/assets/b51c3dba-4cb0-4a53-b067-833eebe447f5" />


---

## ğŸš€ Key Features

- ğŸ“š Upload **multiple PDFs simultaneously**
- ğŸ” Semantic search using **vector embeddings**
- ğŸ§  LLM answers grounded strictly in document context
- ğŸ“Œ **Page-level citations** for every answer
- âš¡ FAISS vector index for fast retrieval
- ğŸ’¾ Persistent index (no re-embedding on every run)
- ğŸ–¥ï¸ Simple and clean **Streamlit UI**

---

## ğŸ§  Why Retrieval-Augmented Generation (RAG)?

Large Language Models are powerful but can hallucinate.

CiteRAG uses a **RAG pipeline**, where:
1. Relevant content is first retrieved from documents
2. The LLM then answers **only using that retrieved content**

This ensures:
- No fabricated answers
- Full traceability
- Verifiable outputs

---

## ğŸ—ï¸ System Architecture
User Question
â”‚
â–¼
Streamlit UI
â”‚
â–¼
FAISS Vector Store (Embeddings)
â”‚
â–¼
Top-K Relevant Chunks
â”‚
â–¼
LLM (ChatOpenAI)
â”‚
â–¼
Answer + Page-Level Citations

Flowchart TD
    
    U[User] -->|Upload PDFs| UI[Streamlit UI]

    UI -->|PDF Files| L[PDF Loader<br/>(PyPDFLoader)]
    L --> M[Page-wise Documents<br/>+ Metadata]

    M --> C[Text Chunking<br/>(RecursiveCharacterTextSplitter)]
    C --> CH[Text Chunks<br/>+ chunk_id]

    CH --> E[Embeddings<br/>(OpenAI text-embedding-3-small)]
    E --> V[FAISS Vector Store]

    V -->|Top-K Similarity Search| R[Relevant Chunks]

    UI -->|User Question| QE[Query Embedding]
    QE --> V

    R --> P[Prompt Assembly<br/>(Context + Question)]
    P --> LLM[ChatOpenAI]

    LLM --> A[Answer]
    A --> CIT[Citations<br/>(PDF + Page No.)]

    CIT --> UI



## ğŸ§© Core RAG Pipeline (`rag_pipeline.py`)

The project follows a clean, modular RAG pipeline:

<img width="4439" height="2574" alt="Untitled-2025-12-24-2147_1 excalidraw" src="https://github.com/user-attachments/assets/04fb7788-4512-4482-84a6-1f56a6d458aa" />


### 1ï¸âƒ£ PDF Loading
- Each PDF is loaded page-by-page
- Metadata added:
  - `source` (file name)
  - `page` (1-based)
  - `doc_id` (unique per document)

### 2ï¸âƒ£ Text Chunking
- Pages are split into overlapping chunks
- Metadata is preserved across chunks
- Each chunk gets a `chunk_id`

### 3ï¸âƒ£ Embeddings
- Chunks are converted into embeddings using:
  - `text-embedding-3-small`

### 4ï¸âƒ£ Vector Storage
- Embeddings are stored in **FAISS**
- Index is saved to disk for reuse

### 5ï¸âƒ£ Retrieval
- Top-K relevant chunks are retrieved based on semantic similarity

### 6ï¸âƒ£ Answer Generation
- LLM answers **only from retrieved chunks**
- Citations are generated from chunk metadata

---

## ğŸ–¥ï¸ Application UI

### ğŸ”¹ Upload PDFs
Users can upload one or more PDF files directly from the UI.

### ğŸ”¹ Ask Questions
Users can ask free-form questions such as:
- *â€œWhat are these documents about?â€*
- *â€œWhat are the key terms and conditions?â€*
- *â€œWhat is the policy maturity date?â€*

### ğŸ”¹ Grounded Answers with Citations
Every answer includes:
- Clear explanation
- Exact PDF name(s)
- Page number(s)

---

## ğŸ“¸ Screenshots

> ğŸ“Œ Add these screenshots to a `/screenshots` folder in your repo

- `screenshots/home.png`
- `screenshots/answer_with_citations.png`
- `screenshots/multi_pdf_upload.png`

(You can add the images later and update the links.)

---

## ğŸ› ï¸ Tech Stack

- **Python**
- **LangChain**
- **OpenAI API**
- **FAISS**
- **Streamlit**
- **PyPDF**
- **dotenv**

---

## ğŸ“‚ Project Structure
multi_pdf_rag_app/
â”‚
â”œâ”€â”€ app.py # Streamlit UI
â”œâ”€â”€ rag_pipeline.py # Core RAG pipeline
â”œâ”€â”€ prompts.py # Prompt templates
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ sample_pdfs/
â”œâ”€â”€ indexes/ # FAISS index (ignored in git)
â”œâ”€â”€ .env.example
â””â”€â”€ .gitignore

## ğŸ” Environment Setup

Create a `.env` file in the project root:
OPENAI_API_KEY=your_api_key_here

Install dependencies: pip install -r requirements.txt

Run the app: streamlit run app.py

## âš ï¸ Limitations

Works best with text-based PDFs (not scanned images)

No document-specific filtering yet (future enhancement)

Retrieval quality depends on chunking strategy

## ğŸš€ Future Improvements

Document-level filters

Highlighted evidence snippets

Confidence scoring per answer

Streaming responses

Authentication & deployment hardening

## ğŸ¯ Learning Outcomes

This project demonstrates:

End-to-end RAG system design

Correct use of embeddings and vector stores

Prompt grounding and hallucination control

Practical LLM application architecture

Production-ready ML engineering practices

## ğŸ™Œ Acknowledgements

Built as part of a hands-on learning exercise to deeply understand Retrieval-Augmented Generation using Langchain.

## â­ If you like this project

Feel free to â­ the repository or fork it for experimentation.







